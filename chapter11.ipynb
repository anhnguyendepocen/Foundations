{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification\n",
    "\n",
    "## Fisher's linear discriminant with equal class covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn import linear_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear discriminant analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb errors=10, error rate=0.05\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "n_samples, n_features = 100, 2\n",
    "mean0, mean1 = np.array([0, 0]), np.array([0, 2])\n",
    "Cov = np.array([[1, .8],[.8, 1]])\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X0 = np.random.multivariate_normal(mean0, Cov, n_samples)\n",
    "X1 = np.random.multivariate_normal(mean1, Cov, n_samples)\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.array([0] * X0.shape[0] + [1] * X1.shape[0])\n",
    "\n",
    "# LDA with scikit-learn\n",
    "lda = LDA()\n",
    "proj = lda.fit(X, y).transform(X)\n",
    "y_pred_lda = lda.predict(X)\n",
    "errors = y_pred_lda != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % (errors.sum(), errors.sum() / len(y_pred_lda)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb errors=10, error rate=0.05\n",
      "[[-5.1516729   5.57303883]]\n"
     ]
    }
   ],
   "source": [
    "logreg = linear_model.LogisticRegression(C=1e8, solver='lbfgs')\n",
    "# This class implements regularized logistic regression. C is the Inverse of regularization strength.\n",
    "# Large value => no regularization.\n",
    "logreg.fit(X, y)\n",
    "y_pred_logreg = logreg.predict(X)\n",
    "errors = y_pred_logreg != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % (errors.sum(), errors.sum() / len(y_pred_logreg)))\n",
    "print(logreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "\n",
    "## Ridge Fisher's linear classification ($l_2$-regularization)\n",
    "\n",
    "## Ridge logistic regression ($l_2$-regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "# Build a classification task using 3 informative features\n",
    "from sklearn import datasets\n",
    "X, y = datasets.make_classification(n_samples=100,n_features=20, n_informative=3, n_redundant=0, \n",
    "                                    n_repeated=0, n_classes=2, random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb errors=26, error rate=0.26\n",
      "[[-0.12899737  0.7579822  -0.01228473 -0.11412421  0.25491221  0.4329847\n",
      "   0.14564739  0.16763962  0.85071394  0.02116803 -0.1611039  -0.0146019\n",
      "  -0.03399884  0.43127728 -0.05831644 -0.0812323   0.15877844  0.29387389\n",
      "   0.54659524  0.03376169]]\n"
     ]
    }
   ],
   "source": [
    "lr = linear_model.LogisticRegression(C=1, solver='lbfgs')\n",
    "# This class implements regularized logistic regression. C is the Inverse of regularization strength.\n",
    "# Large value => no regularization.\n",
    "\n",
    "lr.fit(X, y)\n",
    "y_pred_lr = lr.predict(X)\n",
    "errors = y_pred_lr != y\n",
    "\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % (errors.sum(), errors.sum() / len(y)))\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso logistic regression ($l_1$-regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb errors=25, error rate=0.25\n",
      "[[-0.12618164  0.71723918  0.         -0.00534429  0.2042112   0.38247243\n",
      "   0.07004365  0.06301009  0.76849544  0.         -0.10630664  0.\n",
      "   0.          0.34015127  0.          0.          0.08043682  0.19202706\n",
      "   0.47989086  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "lrl1 = linear_model.LogisticRegression(penalty='l1', solver='saga')\n",
    "# This class implements regularized logistic regression. C is the Inverse of regularization strength.\n",
    "# Large value => no regularization.\n",
    "\n",
    "lrl1.fit(X, y)\n",
    "y_pred_lrl1 = lrl1.predict(X)\n",
    "errors = y_pred_lrl1 != y\n",
    "\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % (errors.sum(), errors.sum() / len(y_pred_lrl1)))\n",
    "print(lrl1.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge linear SVM ($l_2$-regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb errors=26, error rate=0.26\n",
      "[[-0.056121    0.31189589  0.00271961 -0.05149163  0.09940419  0.17726429\n",
      "   0.06519484  0.08921394  0.3533912   0.00601045 -0.06201172 -0.00741169\n",
      "  -0.02156861  0.18272446 -0.02162812 -0.04061099  0.07204358  0.13083485\n",
      "   0.23721453  0.0082412 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amol/.local/lib/python3.5/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svmlin = svm.LinearSVC()\n",
    "# Remark: by default LinearSVC uses squared_hinge as loss\n",
    "svmlin.fit(X, y)\n",
    "y_pred_svmlin = svmlin.predict(X)\n",
    "errors = y_pred_svmlin != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % (errors.sum(), errors.sum() / len(y_pred_svmlin)))\n",
    "print(svmlin.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso SVM ($l_1$-regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb errors=26, error rate=0.26\n",
      "[[-0.0533391   0.29934574  0.         -0.03541637  0.09261429  0.16763337\n",
      "   0.05808022  0.07587758  0.3406516   0.         -0.0555901  -0.00194174\n",
      "  -0.01312517  0.16866053 -0.01450499 -0.02500558  0.06073932  0.11738939\n",
      "   0.22485446  0.00473282]]\n"
     ]
    }
   ],
   "source": [
    "svmlinl1 = svm.LinearSVC(penalty='l1', dual=False)\n",
    "# Remark: by default LinearSVC uses squared_hinge as loss\n",
    "svmlinl1.fit(X, y)\n",
    "y_pred_svmlinl1 = svmlinl1.predict(X)\n",
    "errors = y_pred_svmlinl1 != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % (errors.sum(), errors.sum() / len(y_pred_svmlinl1)))\n",
    "print(svmlinl1.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic-net classification ($l_1$-$l_2$-regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amol/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='elasticnet',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model as lm\n",
    "X, y = datasets.make_classification(n_samples=100, n_features=20, n_informative=3, n_redundant=0, \n",
    "                                    n_repeated=0, n_classes=2, random_state=0, shuffle=False)\n",
    "enetloglike = lm.SGDClassifier(loss=\"log\", penalty=\"elasticnet\", alpha=0.0001, l1_ratio=0.15, \n",
    "                               class_weight='balanced', max_iter=1000, tol=1e-3)\n",
    "enetloglike.fit(X, y)\n",
    "enethinge = lm.SGDClassifier(loss=\"hinge\", penalty=\"elasticnet\", alpha=0.0001, l1_ratio=0.15, \n",
    "                             class_weight='balanced')\n",
    "enethinge.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for classification performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = [0, 1, 0, 0]\n",
    "y_true = [0, 1, 0, 1]\n",
    "metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "# The overall precision an recall\n",
    "metrics.precision_score(y_true, y_pred)\n",
    "metrics.recall_score(y_true, y_pred) \n",
    "\n",
    "# Recalls on individual classes: SEN & SPC\n",
    "recalls = metrics.recall_score(y_true, y_pred, average=None)\n",
    "recalls[0] # is the recall of class 0: specificity\n",
    "recalls[1] # is the recall of class 1: specificity\n",
    "\n",
    "# Balanced accuracy\n",
    "b_acc = recalls.mean()\n",
    "# The overall precision an recall on each individual class\n",
    "p, r, f, s = metrics.precision_recall_fscore_support(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 0 0 0 0 0 0]\n",
      "Recalls: [1. 0.]\n",
      "AUC: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amol/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "score_pred = np.array([.1 ,.2, .3, .4, .5, .6, .7, .8])\n",
    "y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "thres = .9\n",
    "\n",
    "y_pred = (score_pred > thres).astype(int)\n",
    "print(\"Predictions:\", y_pred)\n",
    "metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "# The overall precision an recall on each individual class\n",
    "p, r, f, s = metrics.precision_recall_fscore_support(y_true, y_pred)\n",
    "print(\"Recalls:\", r)\n",
    "\n",
    "# 100% of specificity, 0% of sensitivity\n",
    "# However AUC=1 indicating a perfect separation of the two classes\n",
    "auc = metrics.roc_auc_score(y_true, score_pred)\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalenced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples of class 0 = 250; #samples of class 1 = 250;\n",
      "# No Reweighting balanced dataset\n",
      "SPC: 0.940; SEN: 0.928\n",
      "# => The predictions are balanced in sensitivity and specificity\n",
      "\n",
      "#samples of class 0 = 12; #samples of class 1 = 250;\n",
      "# No Reweighting on imbalanced dataset\n",
      "SPC: 0.750; SEN: 0.996\n",
      "# => Sensitivity >> specificity\n",
      "\n",
      "# Reweighting on imbalanced dataset\n",
      "SPC: 1.000; SEN: 0.980\n",
      "# => The predictions are balanced in sensitivity and specificity\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "X, y = datasets.make_classification(n_samples=500, n_features=5, n_informative=2, n_redundant=0, \n",
    "                                    n_repeated=0, n_classes=2, random_state=1, shuffle=False)\n",
    "\n",
    "print(*[\"#samples of class %i = %i;\" % (lev, np.sum(y == lev)) for lev in np.unique(y)])\n",
    "\n",
    "print('# No Reweighting balanced dataset')\n",
    "lr_inter = linear_model.LogisticRegression(C=1, solver='lbfgs')\n",
    "lr_inter.fit(X, y)\n",
    "p, r, f, s = metrics.precision_recall_fscore_support(y, lr_inter.predict(X))\n",
    "\n",
    "print(\"SPC: %.3f; SEN: %.3f\" % tuple(r))\n",
    "print('# => The predictions are balanced in sensitivity and specificity\\n')\n",
    "\n",
    "# Create imbalanced dataset, by subsampling sample of class 0: keep only 10% of\n",
    "# class 0's samples and all class 1's samples.\n",
    "n0 = int(np.rint(np.sum(y == 0) / 20))\n",
    "subsample_idx = np.concatenate((np.where(y == 0)[0][:n0], np.where(y == 1)[0]))\n",
    "\n",
    "Ximb = X[subsample_idx, :]\n",
    "yimb = y[subsample_idx]\n",
    "print(*[\"#samples of class %i = %i;\" % (lev, np.sum(yimb == lev)) for lev in np.unique(yimb)])\n",
    "\n",
    "print('# No Reweighting on imbalanced dataset')\n",
    "lr_inter = linear_model.LogisticRegression(C=1, solver='lbfgs')\n",
    "lr_inter.fit(Ximb, yimb)\n",
    "p, r, f, s = metrics.precision_recall_fscore_support(yimb, lr_inter.predict(Ximb))\n",
    "\n",
    "print(\"SPC: %.3f; SEN: %.3f\" % tuple(r))\n",
    "print('# => Sensitivity >> specificity\\n')\n",
    "print('# Reweighting on imbalanced dataset')\n",
    "\n",
    "lr_inter_reweight = linear_model.LogisticRegression(C=1,  class_weight=\"balanced\", solver='lbfgs') \n",
    "lr_inter_reweight.fit(Ximb, yimb)\n",
    "p, r, f, s = metrics.precision_recall_fscore_support(yimb, lr_inter_reweight.predict(Ximb))\n",
    "\n",
    "print(\"SPC: %.3f; SEN: %.3f\" % tuple(r))\n",
    "print('# => The predictions are balanced in sensitivity and specificity\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
